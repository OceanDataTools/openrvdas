<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>logger.utils.record_parser API documentation</title>
<meta name="description" content="Tools for parsing NMEA and other text records â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>logger.utils.record_parser</code></h1>
</header>
<section id="section-intro">
<p>Tools for parsing NMEA and other text records.</p>
<p>By default, will load device and device_type definitions from files in
local/devices/*.yaml. Please see documentation in
local/devices/README.md for a description of the format these
definitions should take.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

&#34;&#34;&#34;Tools for parsing NMEA and other text records.

By default, will load device and device_type definitions from files in
local/devices/*.yaml. Please see documentation in
local/devices/README.md for a description of the format these
definitions should take.
&#34;&#34;&#34;
import datetime
import glob
import json
import logging
import parse
import pprint
import sys

# Append openrvdas root to syspath prior to importing openrvdas modules
from os.path import dirname, realpath
sys.path.append(dirname(dirname(dirname(realpath(__file__)))))
from logger.utils import read_config  # noqa: E402
from logger.utils.das_record import DASRecord  # noqa: E402

# Dict of format types that extend the default formats recognized by the
# parse module.
from logger.utils.record_parser_formats import extra_format_types  # noqa: E402

DEFAULT_DEFINITION_PATH = &#39;local/devices/*.yaml&#39;
DEFAULT_RECORD_FORMAT = &#39;{data_id:w} {timestamp:ti} {field_string}&#39;


class RecordParser:
    ############################
    def __init__(self, record_format=None,
                 field_patterns=None, metadata=None,
                 definition_path=DEFAULT_DEFINITION_PATH,
                 return_das_record=False, return_json=False,
                 metadata_interval=None, strip_unprintable=False,
                 quiet=False, prepend_data_id=False, delimiter=&#39;:&#39;):
        &#34;&#34;&#34;Create a parser that will parse field values out of a text record
        and return either a Python dict of data_id, timestamp and fields,
        a JSON encoding of that dict, or a binary DASRecord.
        ```
        record_format - string for parse.parse() to use to break out data_id
            and timestamp from the rest of the message. By default this will
            look for &#39;data_id timestamp field_string&#39;, where &#39;field_string&#39;
            is a str containing the fields to be parsed.

        field_patterns
            If not None, a list of parse patterns to be tried instead
            of looking for device definitions along the definition path,
            or a dict of message_type:[parse_pattern, parse_pattern].

        metadata
            If field_patterns is not None, the metadata to send along with
            data records.

        definition_path - a comma-separated set of file globs in which to look
            for device and device_type definitions with which to parse message.

        return_json - return the parsed fields as a JSON encoded dict

        return_das_record - return the parsed fields as a DASRecord object

        metadata_interval - if not None, include the description, units
            and other metadata pertaining to each field in the returned
            record if those data haven&#39;t been returned in the last
            metadata_interval seconds.

        strip_unprintable
                Strip out and ignore any leading or trailing non-printable binary
                characters in the string to be parsed.

        quiet - if not False, don&#39;t complain when unable to parse a record.

        prepend_data_id - If true prepend the instrument data_id to field_names
            in the record.

        delimiter
            The string to insert between data_id and field_name when prepend_data_id is true.
            Defaults to &#39;:&#39;.
            Not used if prepend_data_id is false.
        ```
        &#34;&#34;&#34;
        self.strip_unprintable = strip_unprintable
        self.quiet = quiet
        self.field_patterns = field_patterns
        self.metadata = metadata or {}
        self.record_format = record_format or DEFAULT_RECORD_FORMAT
        self.compiled_record_format = parse.compile(format=self.record_format,
                                                    extra_types=extra_format_types)
        self.return_das_record = return_das_record
        self.return_json = return_json
        if return_das_record and return_json:
            raise ValueError(&#39;Only one of return_json and return_das_record &#39;
                             &#39;may be true.&#39;)

        self.metadata_interval = metadata_interval
        self.metadata_last_sent = {}
        self.prepend_data_id = prepend_data_id
        self.delimiter = delimiter

        # If we&#39;ve been explicitly given the field_patterns we&#39;re to use for
        # parsing, compile them now. Patterns may either be a list of strings,
        # a dict of strings or a dict of lists of strings.
        if field_patterns:
            self.compiled_field_patterns = self._compile_formats_from_patterns(field_patterns)
            self.metadata = metadata

        # If we&#39;ve not been given field_patterns to use for parsing, read in all
        # the devices and device types to compile them.
        else:
            # Fill in the devices and device_types - NOTE: we won&#39;t be using
            # these if &#39;field_patterns&#39; is provided as an argument.
            definitions = self._new_read_definitions(definition_path)
            self.devices = definitions.get(&#39;devices&#39;, {})
            self.device_types = definitions.get(&#39;device_types&#39;, {})

            # Some limited error checking: make sure that all devices have a
            # defined device_type.
            for device, device_def in self.devices.items():
                device_type = device_def.get(&#39;device_type&#39;, None)
                if not device_type:
                    raise ValueError(&#39;Device definition for &#34;%s&#34; has no declaration of &#39;
                                     &#39;its device_type.&#39; % device)
                if device_type not in self.device_types:
                    raise ValueError(&#39;Device type &#34;%s&#34; (declared in definition of &#34;%s&#34;) &#39;
                                     &#39;is undefined.&#39; % (device_type, device))

            # Compile format definitions so that we can run them more
            # quickly. If format is a single string, normalize it into a list
            # to simplify later code.
            for device_type, device_type_def in self.device_types.items():
                format = device_type_def.get(&#39;format&#39;, None)
                if format is None:
                    raise ValueError(&#39;Device type %s has no format definition&#39;
                                     % device_type)
                compiled_format = self._compile_formats_from_patterns(format)
                self.device_types[device_type][&#39;compiled_format&#39;] = compiled_format

            # Metadata: If we haven&#39;t been handed a dict of metadata, compile it from
            # the devices we&#39;ve read.
            #
            # It&#39;s a map from variable name to the device and device type it
            # came from, along with device type variable and its units and
            # description, if provided in the device type
            # definition. Compiling this information is kind of excruciating
            # and voluminous.
            if not metadata and metadata_interval is not None:
                for device, device_def in self.devices.items():  # e.g. s330
                    device_type_name = device_def.get(&#39;device_type&#39;, None)  # Seapath330
                    if not device_type_name:
                        raise ValueError(&#39;Device definition for &#34;%s&#34; has no declaration of &#39;
                                         &#39;its device_type.&#39; % device)
                    device_type_def = self.device_types.get(device_type_name, None)
                    if not device_type_def:
                        raise ValueError(&#39;Device type &#34;%s&#34; (declared in definition of &#34;%s&#34;)&#39;
                                         &#39; is undefined.&#39; % (device_type_name, device))
                    device_type_fields = device_type_def.get(&#39;fields&#39;, None)
                    if not device_type_fields:
                        raise ValueError(&#39;Device type &#34;%s&#34; has no fields?&#39;
                                         % device_type_name)

                    fields = device_def.get(&#39;fields&#39;, None)
                    if not fields:
                        raise ValueError(&#39;Device &#34;%s&#34; has no fields?!?&#39; % device)

                    # e.g. device_type_field = GPSTime, device_field = S330GPSTime
                    for device_type_field, device_field in fields.items():
                        # e.g. GPSTime: {&#39;units&#39;:..., &#39;description&#39;:...}
                        field_desc = device_type_fields.get(device_type_field, None)
                        if not field_desc:
                            logging.warning(&#39;Device type &#34;%s&#34; has no field corresponding to &#39;
                                            &#39;device field &#34;%s&#34;&#39; % (device_type_name,
                                                                   device_type_field))
                            continue
                        self.metadata[device_field] = {
                            &#39;device&#39;: device,
                            &#39;device_type&#39;: device_type_name,
                            &#39;device_type_field&#39;: device_type_field,
                        }
                        self.metadata[device_field].update(field_desc)

    ############################
    def parse_record(self, record):
        &#34;&#34;&#34;Parse an id-prefixed text record into a Python dict of data_id,
        timestamp and fields.
        &#34;&#34;&#34;
        if not record:
            return None
        if not isinstance(record, str):
            if not self.quiet:
                logging.info(&#39;Record is not a string: &#34;%s&#34;&#39;, record)
            return None
        try:
            # Break record into (by default) data_id, timestamp and field_string
            parsed_record = self.compiled_record_format.parse(record).named
        except (ValueError, AttributeError):
            if not self.quiet:
                logging.warning(&#39;Unable to parse record into &#34;%s&#34;&#39;, self.record_format)
                logging.warning(&#39;Record: %s&#39;, record)
            return None

        # Convert timestamp to numeric, if it&#39;s there
        timestamp = parsed_record.get(&#39;timestamp&#39;, None)
        if timestamp is not None and isinstance(timestamp, datetime.datetime):
            timestamp = timestamp.timestamp()
            parsed_record[&#39;timestamp&#39;] = timestamp

        # Extract the field string we&#39;re going to parse; remove trailing
        # whitespace.
        field_string = parsed_record.get(&#39;field_string&#39;, None)

        # If we don&#39;t have fields, there&#39;s nothing to parse
        if field_string is None:
            if not self.quiet:
                logging.warning(&#39;No field_string found in record &#34;%s&#34;&#39;, record)
            return None

        if self.strip_unprintable:
            field_string = &#39;&#39;.join([c for c in field_string if c.isprintable()])
        field_string = field_string.strip()
        if not field_string:
            if not self.quiet:
                logging.warning(&#39;No field_string found in record &#34;%s&#34;&#39;, record)
            return None

        fields = {}

        # If we&#39;ve been given a set of field_patterns to apply, use the
        # first that matches.
        if self.field_patterns:
            data_id = None
            fields, message_type = self._parse_field_string(field_string,
                                                            self.compiled_field_patterns)
        # If we were given no explicit field_patterns to use, we need to
        # count on the record having a data_id that lets us figure out
        # which device, and therefore which field_patterns to try.
        else:
            data_id = parsed_record.get(&#39;data_id&#39;, None)
            if data_id is None:
                if not self.quiet:
                    logging.warning(&#39;No data id found in record: %s&#39;, record)
                return None
            fields, message_type = self.parse_for_data_id(data_id, field_string)

        # We should now have a dictionary of fields. If not, go home
        if fields is None:
            if not self.quiet:
                logging.warning(&#39;No formats matched field_string &#34;%s&#34;&#39;, field_string)
            return None

        # Some folks want the data_id prepended
        if self.prepend_data_id:
            # This conditional dictates whether fields are stored with just the
            # field_name key, or &lt;data_id&gt;&lt;delimiter&gt;&lt;field_name&gt;
            # Doing some work directly on the fields dict, so we&#39;ll take a copy
            # to loop over.
            fields_copy = fields.copy()
            # Reset the fields dict
            fields = {}
            for field in fields_copy:
                # Determine the new &#34;field_name&#34;
                key = &#39;&#39; + data_id + self.delimiter + field
                # Set the value
                fields[key] = fields_copy[field]

        # Remove raw &#39;field_string&#39; and add parsed &#39;fields&#39; to parsed_record
        del parsed_record[&#39;field_string&#39;]
        parsed_record[&#39;fields&#39;] = fields
        if message_type:
            parsed_record[&#39;message_type&#39;] = message_type

        # If we have parsed fields, see if we also have metadata. Are we
        # supposed to occasionally send it for our variables? Is it time
        # to send it again?
        metadata_fields = {}
        if self.metadata and self.metadata_interval:
            for field_name in fields:
                last_metadata_sent = self.metadata_last_sent.get(field_name, 0)
                time_since_send = timestamp - last_metadata_sent
                if time_since_send &gt; self.metadata_interval:
                    field_metadata = self.metadata.get(field_name, None)
                    if field_metadata:
                        metadata_fields[field_name] = field_metadata
                        self.metadata_last_sent[field_name] = timestamp
        if metadata_fields:
            metadata = {&#39;fields&#39;: metadata_fields}
        else:
            metadata = None

        if metadata:
            parsed_record[&#39;metadata&#39;] = metadata

        logging.debug(&#39;Created parsed record: %s&#39;, pprint.pformat(parsed_record))

        # What are we going to do with the result we&#39;ve created?
        if self.return_das_record:
            try:
                return DASRecord(data_id=data_id, timestamp=timestamp,
                                 message_type=message_type, fields=fields,
                                 metadata=metadata)
            except KeyError:
                return None

        elif self.return_json:
            return json.dumps(parsed_record)
        else:
            return parsed_record

    ############################
    def _parse_field_string(self, field_string, compiled_field_patterns):
        # Default if we don&#39;t match anything
        fields = message_type = None

        # If our pattern(s) are just a single compiled parser, try parsing and
        # return with no message type.
        if isinstance(compiled_field_patterns, parse.Parser):
            result = compiled_field_patterns.parse(field_string)
            if result:
                fields = result.named

        # Else, if it&#39;s a list, try it out on all the elements.
        elif isinstance(compiled_field_patterns, list):
            for pattern in compiled_field_patterns:
                fields, message_type = self._parse_field_string(field_string, pattern)
                if fields is not None:
                    break

        # If it&#39;s a dict, try out on all values, using the key as message type.
        # It&#39;s syntactically possible for the internal set of patterns to have
        # their own message types. Not sure why someone would ever create patterns
        # that did this, but if they do, let that override our base one.
        elif isinstance(compiled_field_patterns, dict):
            for message_type, pattern in compiled_field_patterns.items():
                fields, int_message_type = self._parse_field_string(field_string, pattern)
                message_type = int_message_type or message_type
                if fields is not None:
                    break
        else:
            raise ValueError(&#39;Unexpected pattern type in parser: %s&#39;
                             % type(compiled_field_patterns))

        return fields, message_type

    ############################
    def parse_for_data_id(self, data_id, field_string):
        &#34;&#34;&#34;Look up the device and device type for a data_id. Parse the field_string
        according to those formats. If successful, return a tuple of
        (field_dict, message_type), where field_dict is a dict of
        {field_name: field_value}. Return ({}, None) if unable to match a format pattern.
        &#34;&#34;&#34;
        failure_values = (None, None)
        if not self.devices:
            logging.warning(&#39;RecordParser has no device definitions; unable to parse!&#39;)
            return failure_values

        # Get device and device_type definitions for data_id
        device = self.devices.get(data_id, None)
        if not device:
            if not self.quiet:
                logging.warning(&#39;Unrecognized data id &#34;%s&#34;, field string: %s&#39;,
                                data_id, field_string)
                logging.warning(&#39;Known data ids are: &#34;%s&#34;&#39;,
                                &#39;, &#39;.join(self.devices.keys()))
            return failure_values

        device_type = device.get(&#39;device_type&#39;, None)
        if not device_type:
            if not self.quiet:
                logging.error(&#39;Internal error: No &#34;device_type&#34; for device %s!&#39;, device)
            return failure_values

        device_definition = self.device_types.get(device_type, None)
        if not device_definition:
            if not self.quiet:
                logging.error(&#39;No definition found for device_type &#34;%s&#34;&#39;, device_type)
            return failure_values

        compiled_format_patterns = device_definition.get(&#39;compiled_format&#39;, None)
        parsed_fields, message_type = \
            self._parse_field_string(field_string, compiled_format_patterns)

        # Did we get anything?
        if parsed_fields is None:
            if not self.quiet:
                logging.warning(&#39;No formats matched field_string &#34;%s&#34;&#39;, field_string)
            return failure_values

        logging.debug(&#39;Got fields: %s&#39;, pprint.pformat(parsed_fields))

        # Finally, convert field values to variable names specific to device
        device_fields = device.get(&#39;fields&#39;, None)
        if not device_fields:
            if not self.quiet:
                logging.error(&#39;No &#34;fields&#34; definition found for device %s&#39;, data_id)
            return failure_values

        # Assign field values to the appropriate named variable.
        fields = {}
        for field_name, value in parsed_fields.items():
            variable_name = device_fields.get(field_name, None)
            # None means we&#39;re not supposed to report it.
            if variable_name is None:
                continue
            # None means we didn&#39;t have a value for this field; omit it.
            if value is None:
                continue
            # If it&#39;s a datetime, convert to numeric timestamp
            if isinstance(value, datetime.datetime):
                value = value.timestamp()
            fields[variable_name] = value

        logging.debug(&#39;Got fields: %s&#39;, pprint.pformat(fields))
        return fields, message_type

    ############################
    def _compile_formats_from_patterns(self, field_patterns):
        &#34;&#34;&#34;Return a list/dict of patterns compiled from the
        str/list/dict of passed field_patterns.
        &#34;&#34;&#34;
        if isinstance(field_patterns, str):
            return [parse.compile(format=field_patterns,
                                  extra_types=extra_format_types)]
        elif isinstance(field_patterns, list):
            return [parse.compile(format=p, extra_types=extra_format_types)
                    for p in field_patterns]
        elif isinstance(field_patterns, dict):
            compiled_field_patterns = {}
            for message_type, message_pattern in field_patterns.items():
                compiled_patterns = self._compile_formats_from_patterns(message_pattern)
                compiled_field_patterns[message_type] = compiled_patterns
            return compiled_field_patterns

        else:
            raise ValueError(&#39;Passed field_patterns must be str, list or dict. Found %s: %s&#39;
                             % (type(field_patterns), str(field_patterns)))

    ############################
    def _read_definitions(self, filespec_paths):
        &#34;&#34;&#34;Read the files on the filespec_paths and return dictionary of
        accumulated definitions.
        &#34;&#34;&#34;
        definitions = {}
        for filespec in filespec_paths.split(&#39;,&#39;):
            filenames = glob.glob(filespec)
            if not filenames:
                logging.warning(&#39;No files match definition file spec &#34;%s&#34;&#39;, filespec)

            for filename in filenames:
                file_definitions = read_config.read_config(filename)

                for new_def_name, new_def in file_definitions.items():
                    if new_def_name in definitions:
                        logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                        new_def_name, filename)
                    definitions[new_def_name] = new_def
        return definitions

    ############################
    def _new_read_definitions(self, filespec_paths, definitions=None):
        &#34;&#34;&#34;Read the files on the filespec_paths and return dictionary of
        accumulated definitions.

        filespec_paths - a list of possibly-globbed filespecs to be read

        definitions - optional dict of pre-existing definitions that will
                      be added to. Typically this will be omitted on a base call,
                      but may be added to when recursing. Passing it in allows
                      flagging when items are defined more than once.
        &#34;&#34;&#34;
        # If nothing was passed in, start with base case.
        definitions = definitions or {&#39;devices&#39;: {}, &#39;device_types&#39;: {}}

        for filespec in filespec_paths.split(&#39;,&#39;):
            filenames = glob.glob(filespec)
            if not filenames:
                logging.warning(&#39;No files match definition file spec &#34;%s&#34;&#39;, filespec)

            for filename in filenames:
                file_definitions = read_config.read_config(filename)

                for key, val in file_definitions.items():
                    # If we have a dict of device definitions, copy them into the
                    # &#39;devices&#39; key of our definitions.
                    if key == &#39;devices&#39;:
                        if not isinstance(val, dict):
                            logging.error(&#39;&#34;devices&#34; values in file %s must be dict. &#39;
                                          &#39;Found type &#34;%s&#34;&#39;, filename, type(val))
                            return None

                        for device_name, device_def in val.items():
                            if device_name in definitions[&#39;devices&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                device_name, filename)
                            definitions[&#39;devices&#39;][device_name] = device_def

                    # If we have a dict of device_type definitions, copy them into the
                    # &#39;device_types&#39; key of our definitions.
                    elif key == &#39;device_types&#39;:
                        if not isinstance(val, dict):
                            logging.error(&#39;&#34;device_typess&#34; values in file %s must be dict. &#39;
                                          &#39;Found type &#34;%s&#34;&#39;, filename, type(val))
                            return None

                        for device_type_name, device_type_def in val.items():
                            if device_type_name in definitions[&#39;device_types&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                device_type_name, filename)
                            definitions[&#39;device_types&#39;][device_type_name] = device_type_def

                    # If we&#39;re including other files, recurse inelegantly
                    elif key == &#39;includes&#39;:
                        if not type(val) in [str, list]:
                            logging.error(&#39;&#34;includes&#34; values in file %s must be either &#39;
                                          &#39;a list or a simple string. Found type &#34;%s&#34;&#39;,
                                          filename, type(val))
                            return None

                        if isinstance(val, str):
                            val = [val]
                        for filespec in val:
                            new_defs = self._new_read_definitions(filespec, definitions)
                            definitions[&#39;devices&#39;].update(new_defs.get(&#39;devices&#39;, {}))
                            definitions[&#39;device_types&#39;].update(new_defs.get(&#39;device_types&#39;, {}))

                    # If it&#39;s not an includes/devices/device_types def, assume
                    # it&#39;s a (deprecated) top-level device or device_type
                    # definition. Try adding it to the right place.
                    else:
                        category = val.get(&#39;category&#39;, None)
                        if category not in [&#39;device&#39;, &#39;device_type&#39;]:
                            logging.warning(&#39;Top-level definition &#34;%s&#34; in file %s is not &#39;
                                            &#39;category &#34;device&#34; or &#34;device_type&#34;. &#39;
                                            &#39;Category is &#34;%s&#34; - ignoring&#39;, category)
                            continue
                        if category == &#39;device&#39;:
                            if key in definitions[&#39;devices&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                key, filename)
                            definitions[&#39;devices&#39;][key] = val
                        else:
                            if key in definitions[&#39;device_types&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                key, filename)
                            definitions[&#39;device_types&#39;][key] = val

        # Finally, return the accumulated definitions
        return definitions</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="logger.utils.record_parser.RecordParser"><code class="flex name class">
<span>class <span class="ident">RecordParser</span></span>
<span>(</span><span>record_format=None, field_patterns=None, metadata=None, definition_path='local/devices/*.yaml', return_das_record=False, return_json=False, metadata_interval=None, strip_unprintable=False, quiet=False, prepend_data_id=False, delimiter=':')</span>
</code></dt>
<dd>
<div class="desc"><p>Create a parser that will parse field values out of a text record
and return either a Python dict of data_id, timestamp and fields,
a JSON encoding of that dict, or a binary DASRecord.</p>
<pre><code>record_format - string for parse.parse() to use to break out data_id
    and timestamp from the rest of the message. By default this will
    look for 'data_id timestamp field_string', where 'field_string'
    is a str containing the fields to be parsed.

field_patterns
    If not None, a list of parse patterns to be tried instead
    of looking for device definitions along the definition path,
    or a dict of message_type:[parse_pattern, parse_pattern].

metadata
    If field_patterns is not None, the metadata to send along with
    data records.

definition_path - a comma-separated set of file globs in which to look
    for device and device_type definitions with which to parse message.

return_json - return the parsed fields as a JSON encoded dict

return_das_record - return the parsed fields as a DASRecord object

metadata_interval - if not None, include the description, units
    and other metadata pertaining to each field in the returned
    record if those data haven't been returned in the last
    metadata_interval seconds.

strip_unprintable
        Strip out and ignore any leading or trailing non-printable binary
        characters in the string to be parsed.

quiet - if not False, don't complain when unable to parse a record.

prepend_data_id - If true prepend the instrument data_id to field_names
    in the record.

delimiter
    The string to insert between data_id and field_name when prepend_data_id is true.
    Defaults to ':'.
    Not used if prepend_data_id is false.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RecordParser:
    ############################
    def __init__(self, record_format=None,
                 field_patterns=None, metadata=None,
                 definition_path=DEFAULT_DEFINITION_PATH,
                 return_das_record=False, return_json=False,
                 metadata_interval=None, strip_unprintable=False,
                 quiet=False, prepend_data_id=False, delimiter=&#39;:&#39;):
        &#34;&#34;&#34;Create a parser that will parse field values out of a text record
        and return either a Python dict of data_id, timestamp and fields,
        a JSON encoding of that dict, or a binary DASRecord.
        ```
        record_format - string for parse.parse() to use to break out data_id
            and timestamp from the rest of the message. By default this will
            look for &#39;data_id timestamp field_string&#39;, where &#39;field_string&#39;
            is a str containing the fields to be parsed.

        field_patterns
            If not None, a list of parse patterns to be tried instead
            of looking for device definitions along the definition path,
            or a dict of message_type:[parse_pattern, parse_pattern].

        metadata
            If field_patterns is not None, the metadata to send along with
            data records.

        definition_path - a comma-separated set of file globs in which to look
            for device and device_type definitions with which to parse message.

        return_json - return the parsed fields as a JSON encoded dict

        return_das_record - return the parsed fields as a DASRecord object

        metadata_interval - if not None, include the description, units
            and other metadata pertaining to each field in the returned
            record if those data haven&#39;t been returned in the last
            metadata_interval seconds.

        strip_unprintable
                Strip out and ignore any leading or trailing non-printable binary
                characters in the string to be parsed.

        quiet - if not False, don&#39;t complain when unable to parse a record.

        prepend_data_id - If true prepend the instrument data_id to field_names
            in the record.

        delimiter
            The string to insert between data_id and field_name when prepend_data_id is true.
            Defaults to &#39;:&#39;.
            Not used if prepend_data_id is false.
        ```
        &#34;&#34;&#34;
        self.strip_unprintable = strip_unprintable
        self.quiet = quiet
        self.field_patterns = field_patterns
        self.metadata = metadata or {}
        self.record_format = record_format or DEFAULT_RECORD_FORMAT
        self.compiled_record_format = parse.compile(format=self.record_format,
                                                    extra_types=extra_format_types)
        self.return_das_record = return_das_record
        self.return_json = return_json
        if return_das_record and return_json:
            raise ValueError(&#39;Only one of return_json and return_das_record &#39;
                             &#39;may be true.&#39;)

        self.metadata_interval = metadata_interval
        self.metadata_last_sent = {}
        self.prepend_data_id = prepend_data_id
        self.delimiter = delimiter

        # If we&#39;ve been explicitly given the field_patterns we&#39;re to use for
        # parsing, compile them now. Patterns may either be a list of strings,
        # a dict of strings or a dict of lists of strings.
        if field_patterns:
            self.compiled_field_patterns = self._compile_formats_from_patterns(field_patterns)
            self.metadata = metadata

        # If we&#39;ve not been given field_patterns to use for parsing, read in all
        # the devices and device types to compile them.
        else:
            # Fill in the devices and device_types - NOTE: we won&#39;t be using
            # these if &#39;field_patterns&#39; is provided as an argument.
            definitions = self._new_read_definitions(definition_path)
            self.devices = definitions.get(&#39;devices&#39;, {})
            self.device_types = definitions.get(&#39;device_types&#39;, {})

            # Some limited error checking: make sure that all devices have a
            # defined device_type.
            for device, device_def in self.devices.items():
                device_type = device_def.get(&#39;device_type&#39;, None)
                if not device_type:
                    raise ValueError(&#39;Device definition for &#34;%s&#34; has no declaration of &#39;
                                     &#39;its device_type.&#39; % device)
                if device_type not in self.device_types:
                    raise ValueError(&#39;Device type &#34;%s&#34; (declared in definition of &#34;%s&#34;) &#39;
                                     &#39;is undefined.&#39; % (device_type, device))

            # Compile format definitions so that we can run them more
            # quickly. If format is a single string, normalize it into a list
            # to simplify later code.
            for device_type, device_type_def in self.device_types.items():
                format = device_type_def.get(&#39;format&#39;, None)
                if format is None:
                    raise ValueError(&#39;Device type %s has no format definition&#39;
                                     % device_type)
                compiled_format = self._compile_formats_from_patterns(format)
                self.device_types[device_type][&#39;compiled_format&#39;] = compiled_format

            # Metadata: If we haven&#39;t been handed a dict of metadata, compile it from
            # the devices we&#39;ve read.
            #
            # It&#39;s a map from variable name to the device and device type it
            # came from, along with device type variable and its units and
            # description, if provided in the device type
            # definition. Compiling this information is kind of excruciating
            # and voluminous.
            if not metadata and metadata_interval is not None:
                for device, device_def in self.devices.items():  # e.g. s330
                    device_type_name = device_def.get(&#39;device_type&#39;, None)  # Seapath330
                    if not device_type_name:
                        raise ValueError(&#39;Device definition for &#34;%s&#34; has no declaration of &#39;
                                         &#39;its device_type.&#39; % device)
                    device_type_def = self.device_types.get(device_type_name, None)
                    if not device_type_def:
                        raise ValueError(&#39;Device type &#34;%s&#34; (declared in definition of &#34;%s&#34;)&#39;
                                         &#39; is undefined.&#39; % (device_type_name, device))
                    device_type_fields = device_type_def.get(&#39;fields&#39;, None)
                    if not device_type_fields:
                        raise ValueError(&#39;Device type &#34;%s&#34; has no fields?&#39;
                                         % device_type_name)

                    fields = device_def.get(&#39;fields&#39;, None)
                    if not fields:
                        raise ValueError(&#39;Device &#34;%s&#34; has no fields?!?&#39; % device)

                    # e.g. device_type_field = GPSTime, device_field = S330GPSTime
                    for device_type_field, device_field in fields.items():
                        # e.g. GPSTime: {&#39;units&#39;:..., &#39;description&#39;:...}
                        field_desc = device_type_fields.get(device_type_field, None)
                        if not field_desc:
                            logging.warning(&#39;Device type &#34;%s&#34; has no field corresponding to &#39;
                                            &#39;device field &#34;%s&#34;&#39; % (device_type_name,
                                                                   device_type_field))
                            continue
                        self.metadata[device_field] = {
                            &#39;device&#39;: device,
                            &#39;device_type&#39;: device_type_name,
                            &#39;device_type_field&#39;: device_type_field,
                        }
                        self.metadata[device_field].update(field_desc)

    ############################
    def parse_record(self, record):
        &#34;&#34;&#34;Parse an id-prefixed text record into a Python dict of data_id,
        timestamp and fields.
        &#34;&#34;&#34;
        if not record:
            return None
        if not isinstance(record, str):
            if not self.quiet:
                logging.info(&#39;Record is not a string: &#34;%s&#34;&#39;, record)
            return None
        try:
            # Break record into (by default) data_id, timestamp and field_string
            parsed_record = self.compiled_record_format.parse(record).named
        except (ValueError, AttributeError):
            if not self.quiet:
                logging.warning(&#39;Unable to parse record into &#34;%s&#34;&#39;, self.record_format)
                logging.warning(&#39;Record: %s&#39;, record)
            return None

        # Convert timestamp to numeric, if it&#39;s there
        timestamp = parsed_record.get(&#39;timestamp&#39;, None)
        if timestamp is not None and isinstance(timestamp, datetime.datetime):
            timestamp = timestamp.timestamp()
            parsed_record[&#39;timestamp&#39;] = timestamp

        # Extract the field string we&#39;re going to parse; remove trailing
        # whitespace.
        field_string = parsed_record.get(&#39;field_string&#39;, None)

        # If we don&#39;t have fields, there&#39;s nothing to parse
        if field_string is None:
            if not self.quiet:
                logging.warning(&#39;No field_string found in record &#34;%s&#34;&#39;, record)
            return None

        if self.strip_unprintable:
            field_string = &#39;&#39;.join([c for c in field_string if c.isprintable()])
        field_string = field_string.strip()
        if not field_string:
            if not self.quiet:
                logging.warning(&#39;No field_string found in record &#34;%s&#34;&#39;, record)
            return None

        fields = {}

        # If we&#39;ve been given a set of field_patterns to apply, use the
        # first that matches.
        if self.field_patterns:
            data_id = None
            fields, message_type = self._parse_field_string(field_string,
                                                            self.compiled_field_patterns)
        # If we were given no explicit field_patterns to use, we need to
        # count on the record having a data_id that lets us figure out
        # which device, and therefore which field_patterns to try.
        else:
            data_id = parsed_record.get(&#39;data_id&#39;, None)
            if data_id is None:
                if not self.quiet:
                    logging.warning(&#39;No data id found in record: %s&#39;, record)
                return None
            fields, message_type = self.parse_for_data_id(data_id, field_string)

        # We should now have a dictionary of fields. If not, go home
        if fields is None:
            if not self.quiet:
                logging.warning(&#39;No formats matched field_string &#34;%s&#34;&#39;, field_string)
            return None

        # Some folks want the data_id prepended
        if self.prepend_data_id:
            # This conditional dictates whether fields are stored with just the
            # field_name key, or &lt;data_id&gt;&lt;delimiter&gt;&lt;field_name&gt;
            # Doing some work directly on the fields dict, so we&#39;ll take a copy
            # to loop over.
            fields_copy = fields.copy()
            # Reset the fields dict
            fields = {}
            for field in fields_copy:
                # Determine the new &#34;field_name&#34;
                key = &#39;&#39; + data_id + self.delimiter + field
                # Set the value
                fields[key] = fields_copy[field]

        # Remove raw &#39;field_string&#39; and add parsed &#39;fields&#39; to parsed_record
        del parsed_record[&#39;field_string&#39;]
        parsed_record[&#39;fields&#39;] = fields
        if message_type:
            parsed_record[&#39;message_type&#39;] = message_type

        # If we have parsed fields, see if we also have metadata. Are we
        # supposed to occasionally send it for our variables? Is it time
        # to send it again?
        metadata_fields = {}
        if self.metadata and self.metadata_interval:
            for field_name in fields:
                last_metadata_sent = self.metadata_last_sent.get(field_name, 0)
                time_since_send = timestamp - last_metadata_sent
                if time_since_send &gt; self.metadata_interval:
                    field_metadata = self.metadata.get(field_name, None)
                    if field_metadata:
                        metadata_fields[field_name] = field_metadata
                        self.metadata_last_sent[field_name] = timestamp
        if metadata_fields:
            metadata = {&#39;fields&#39;: metadata_fields}
        else:
            metadata = None

        if metadata:
            parsed_record[&#39;metadata&#39;] = metadata

        logging.debug(&#39;Created parsed record: %s&#39;, pprint.pformat(parsed_record))

        # What are we going to do with the result we&#39;ve created?
        if self.return_das_record:
            try:
                return DASRecord(data_id=data_id, timestamp=timestamp,
                                 message_type=message_type, fields=fields,
                                 metadata=metadata)
            except KeyError:
                return None

        elif self.return_json:
            return json.dumps(parsed_record)
        else:
            return parsed_record

    ############################
    def _parse_field_string(self, field_string, compiled_field_patterns):
        # Default if we don&#39;t match anything
        fields = message_type = None

        # If our pattern(s) are just a single compiled parser, try parsing and
        # return with no message type.
        if isinstance(compiled_field_patterns, parse.Parser):
            result = compiled_field_patterns.parse(field_string)
            if result:
                fields = result.named

        # Else, if it&#39;s a list, try it out on all the elements.
        elif isinstance(compiled_field_patterns, list):
            for pattern in compiled_field_patterns:
                fields, message_type = self._parse_field_string(field_string, pattern)
                if fields is not None:
                    break

        # If it&#39;s a dict, try out on all values, using the key as message type.
        # It&#39;s syntactically possible for the internal set of patterns to have
        # their own message types. Not sure why someone would ever create patterns
        # that did this, but if they do, let that override our base one.
        elif isinstance(compiled_field_patterns, dict):
            for message_type, pattern in compiled_field_patterns.items():
                fields, int_message_type = self._parse_field_string(field_string, pattern)
                message_type = int_message_type or message_type
                if fields is not None:
                    break
        else:
            raise ValueError(&#39;Unexpected pattern type in parser: %s&#39;
                             % type(compiled_field_patterns))

        return fields, message_type

    ############################
    def parse_for_data_id(self, data_id, field_string):
        &#34;&#34;&#34;Look up the device and device type for a data_id. Parse the field_string
        according to those formats. If successful, return a tuple of
        (field_dict, message_type), where field_dict is a dict of
        {field_name: field_value}. Return ({}, None) if unable to match a format pattern.
        &#34;&#34;&#34;
        failure_values = (None, None)
        if not self.devices:
            logging.warning(&#39;RecordParser has no device definitions; unable to parse!&#39;)
            return failure_values

        # Get device and device_type definitions for data_id
        device = self.devices.get(data_id, None)
        if not device:
            if not self.quiet:
                logging.warning(&#39;Unrecognized data id &#34;%s&#34;, field string: %s&#39;,
                                data_id, field_string)
                logging.warning(&#39;Known data ids are: &#34;%s&#34;&#39;,
                                &#39;, &#39;.join(self.devices.keys()))
            return failure_values

        device_type = device.get(&#39;device_type&#39;, None)
        if not device_type:
            if not self.quiet:
                logging.error(&#39;Internal error: No &#34;device_type&#34; for device %s!&#39;, device)
            return failure_values

        device_definition = self.device_types.get(device_type, None)
        if not device_definition:
            if not self.quiet:
                logging.error(&#39;No definition found for device_type &#34;%s&#34;&#39;, device_type)
            return failure_values

        compiled_format_patterns = device_definition.get(&#39;compiled_format&#39;, None)
        parsed_fields, message_type = \
            self._parse_field_string(field_string, compiled_format_patterns)

        # Did we get anything?
        if parsed_fields is None:
            if not self.quiet:
                logging.warning(&#39;No formats matched field_string &#34;%s&#34;&#39;, field_string)
            return failure_values

        logging.debug(&#39;Got fields: %s&#39;, pprint.pformat(parsed_fields))

        # Finally, convert field values to variable names specific to device
        device_fields = device.get(&#39;fields&#39;, None)
        if not device_fields:
            if not self.quiet:
                logging.error(&#39;No &#34;fields&#34; definition found for device %s&#39;, data_id)
            return failure_values

        # Assign field values to the appropriate named variable.
        fields = {}
        for field_name, value in parsed_fields.items():
            variable_name = device_fields.get(field_name, None)
            # None means we&#39;re not supposed to report it.
            if variable_name is None:
                continue
            # None means we didn&#39;t have a value for this field; omit it.
            if value is None:
                continue
            # If it&#39;s a datetime, convert to numeric timestamp
            if isinstance(value, datetime.datetime):
                value = value.timestamp()
            fields[variable_name] = value

        logging.debug(&#39;Got fields: %s&#39;, pprint.pformat(fields))
        return fields, message_type

    ############################
    def _compile_formats_from_patterns(self, field_patterns):
        &#34;&#34;&#34;Return a list/dict of patterns compiled from the
        str/list/dict of passed field_patterns.
        &#34;&#34;&#34;
        if isinstance(field_patterns, str):
            return [parse.compile(format=field_patterns,
                                  extra_types=extra_format_types)]
        elif isinstance(field_patterns, list):
            return [parse.compile(format=p, extra_types=extra_format_types)
                    for p in field_patterns]
        elif isinstance(field_patterns, dict):
            compiled_field_patterns = {}
            for message_type, message_pattern in field_patterns.items():
                compiled_patterns = self._compile_formats_from_patterns(message_pattern)
                compiled_field_patterns[message_type] = compiled_patterns
            return compiled_field_patterns

        else:
            raise ValueError(&#39;Passed field_patterns must be str, list or dict. Found %s: %s&#39;
                             % (type(field_patterns), str(field_patterns)))

    ############################
    def _read_definitions(self, filespec_paths):
        &#34;&#34;&#34;Read the files on the filespec_paths and return dictionary of
        accumulated definitions.
        &#34;&#34;&#34;
        definitions = {}
        for filespec in filespec_paths.split(&#39;,&#39;):
            filenames = glob.glob(filespec)
            if not filenames:
                logging.warning(&#39;No files match definition file spec &#34;%s&#34;&#39;, filespec)

            for filename in filenames:
                file_definitions = read_config.read_config(filename)

                for new_def_name, new_def in file_definitions.items():
                    if new_def_name in definitions:
                        logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                        new_def_name, filename)
                    definitions[new_def_name] = new_def
        return definitions

    ############################
    def _new_read_definitions(self, filespec_paths, definitions=None):
        &#34;&#34;&#34;Read the files on the filespec_paths and return dictionary of
        accumulated definitions.

        filespec_paths - a list of possibly-globbed filespecs to be read

        definitions - optional dict of pre-existing definitions that will
                      be added to. Typically this will be omitted on a base call,
                      but may be added to when recursing. Passing it in allows
                      flagging when items are defined more than once.
        &#34;&#34;&#34;
        # If nothing was passed in, start with base case.
        definitions = definitions or {&#39;devices&#39;: {}, &#39;device_types&#39;: {}}

        for filespec in filespec_paths.split(&#39;,&#39;):
            filenames = glob.glob(filespec)
            if not filenames:
                logging.warning(&#39;No files match definition file spec &#34;%s&#34;&#39;, filespec)

            for filename in filenames:
                file_definitions = read_config.read_config(filename)

                for key, val in file_definitions.items():
                    # If we have a dict of device definitions, copy them into the
                    # &#39;devices&#39; key of our definitions.
                    if key == &#39;devices&#39;:
                        if not isinstance(val, dict):
                            logging.error(&#39;&#34;devices&#34; values in file %s must be dict. &#39;
                                          &#39;Found type &#34;%s&#34;&#39;, filename, type(val))
                            return None

                        for device_name, device_def in val.items():
                            if device_name in definitions[&#39;devices&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                device_name, filename)
                            definitions[&#39;devices&#39;][device_name] = device_def

                    # If we have a dict of device_type definitions, copy them into the
                    # &#39;device_types&#39; key of our definitions.
                    elif key == &#39;device_types&#39;:
                        if not isinstance(val, dict):
                            logging.error(&#39;&#34;device_typess&#34; values in file %s must be dict. &#39;
                                          &#39;Found type &#34;%s&#34;&#39;, filename, type(val))
                            return None

                        for device_type_name, device_type_def in val.items():
                            if device_type_name in definitions[&#39;device_types&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                device_type_name, filename)
                            definitions[&#39;device_types&#39;][device_type_name] = device_type_def

                    # If we&#39;re including other files, recurse inelegantly
                    elif key == &#39;includes&#39;:
                        if not type(val) in [str, list]:
                            logging.error(&#39;&#34;includes&#34; values in file %s must be either &#39;
                                          &#39;a list or a simple string. Found type &#34;%s&#34;&#39;,
                                          filename, type(val))
                            return None

                        if isinstance(val, str):
                            val = [val]
                        for filespec in val:
                            new_defs = self._new_read_definitions(filespec, definitions)
                            definitions[&#39;devices&#39;].update(new_defs.get(&#39;devices&#39;, {}))
                            definitions[&#39;device_types&#39;].update(new_defs.get(&#39;device_types&#39;, {}))

                    # If it&#39;s not an includes/devices/device_types def, assume
                    # it&#39;s a (deprecated) top-level device or device_type
                    # definition. Try adding it to the right place.
                    else:
                        category = val.get(&#39;category&#39;, None)
                        if category not in [&#39;device&#39;, &#39;device_type&#39;]:
                            logging.warning(&#39;Top-level definition &#34;%s&#34; in file %s is not &#39;
                                            &#39;category &#34;device&#34; or &#34;device_type&#34;. &#39;
                                            &#39;Category is &#34;%s&#34; - ignoring&#39;, category)
                            continue
                        if category == &#39;device&#39;:
                            if key in definitions[&#39;devices&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                key, filename)
                            definitions[&#39;devices&#39;][key] = val
                        else:
                            if key in definitions[&#39;device_types&#39;]:
                                logging.warning(&#39;Duplicate definition for &#34;%s&#34; found in %s&#39;,
                                                key, filename)
                            definitions[&#39;device_types&#39;][key] = val

        # Finally, return the accumulated definitions
        return definitions</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="logger.utils.record_parser.RecordParser.parse_for_data_id"><code class="name flex">
<span>def <span class="ident">parse_for_data_id</span></span>(<span>self, data_id, field_string)</span>
</code></dt>
<dd>
<div class="desc"><p>Look up the device and device type for a data_id. Parse the field_string
according to those formats. If successful, return a tuple of
(field_dict, message_type), where field_dict is a dict of
{field_name: field_value}. Return ({}, None) if unable to match a format pattern.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_for_data_id(self, data_id, field_string):
    &#34;&#34;&#34;Look up the device and device type for a data_id. Parse the field_string
    according to those formats. If successful, return a tuple of
    (field_dict, message_type), where field_dict is a dict of
    {field_name: field_value}. Return ({}, None) if unable to match a format pattern.
    &#34;&#34;&#34;
    failure_values = (None, None)
    if not self.devices:
        logging.warning(&#39;RecordParser has no device definitions; unable to parse!&#39;)
        return failure_values

    # Get device and device_type definitions for data_id
    device = self.devices.get(data_id, None)
    if not device:
        if not self.quiet:
            logging.warning(&#39;Unrecognized data id &#34;%s&#34;, field string: %s&#39;,
                            data_id, field_string)
            logging.warning(&#39;Known data ids are: &#34;%s&#34;&#39;,
                            &#39;, &#39;.join(self.devices.keys()))
        return failure_values

    device_type = device.get(&#39;device_type&#39;, None)
    if not device_type:
        if not self.quiet:
            logging.error(&#39;Internal error: No &#34;device_type&#34; for device %s!&#39;, device)
        return failure_values

    device_definition = self.device_types.get(device_type, None)
    if not device_definition:
        if not self.quiet:
            logging.error(&#39;No definition found for device_type &#34;%s&#34;&#39;, device_type)
        return failure_values

    compiled_format_patterns = device_definition.get(&#39;compiled_format&#39;, None)
    parsed_fields, message_type = \
        self._parse_field_string(field_string, compiled_format_patterns)

    # Did we get anything?
    if parsed_fields is None:
        if not self.quiet:
            logging.warning(&#39;No formats matched field_string &#34;%s&#34;&#39;, field_string)
        return failure_values

    logging.debug(&#39;Got fields: %s&#39;, pprint.pformat(parsed_fields))

    # Finally, convert field values to variable names specific to device
    device_fields = device.get(&#39;fields&#39;, None)
    if not device_fields:
        if not self.quiet:
            logging.error(&#39;No &#34;fields&#34; definition found for device %s&#39;, data_id)
        return failure_values

    # Assign field values to the appropriate named variable.
    fields = {}
    for field_name, value in parsed_fields.items():
        variable_name = device_fields.get(field_name, None)
        # None means we&#39;re not supposed to report it.
        if variable_name is None:
            continue
        # None means we didn&#39;t have a value for this field; omit it.
        if value is None:
            continue
        # If it&#39;s a datetime, convert to numeric timestamp
        if isinstance(value, datetime.datetime):
            value = value.timestamp()
        fields[variable_name] = value

    logging.debug(&#39;Got fields: %s&#39;, pprint.pformat(fields))
    return fields, message_type</code></pre>
</details>
</dd>
<dt id="logger.utils.record_parser.RecordParser.parse_record"><code class="name flex">
<span>def <span class="ident">parse_record</span></span>(<span>self, record)</span>
</code></dt>
<dd>
<div class="desc"><p>Parse an id-prefixed text record into a Python dict of data_id,
timestamp and fields.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_record(self, record):
    &#34;&#34;&#34;Parse an id-prefixed text record into a Python dict of data_id,
    timestamp and fields.
    &#34;&#34;&#34;
    if not record:
        return None
    if not isinstance(record, str):
        if not self.quiet:
            logging.info(&#39;Record is not a string: &#34;%s&#34;&#39;, record)
        return None
    try:
        # Break record into (by default) data_id, timestamp and field_string
        parsed_record = self.compiled_record_format.parse(record).named
    except (ValueError, AttributeError):
        if not self.quiet:
            logging.warning(&#39;Unable to parse record into &#34;%s&#34;&#39;, self.record_format)
            logging.warning(&#39;Record: %s&#39;, record)
        return None

    # Convert timestamp to numeric, if it&#39;s there
    timestamp = parsed_record.get(&#39;timestamp&#39;, None)
    if timestamp is not None and isinstance(timestamp, datetime.datetime):
        timestamp = timestamp.timestamp()
        parsed_record[&#39;timestamp&#39;] = timestamp

    # Extract the field string we&#39;re going to parse; remove trailing
    # whitespace.
    field_string = parsed_record.get(&#39;field_string&#39;, None)

    # If we don&#39;t have fields, there&#39;s nothing to parse
    if field_string is None:
        if not self.quiet:
            logging.warning(&#39;No field_string found in record &#34;%s&#34;&#39;, record)
        return None

    if self.strip_unprintable:
        field_string = &#39;&#39;.join([c for c in field_string if c.isprintable()])
    field_string = field_string.strip()
    if not field_string:
        if not self.quiet:
            logging.warning(&#39;No field_string found in record &#34;%s&#34;&#39;, record)
        return None

    fields = {}

    # If we&#39;ve been given a set of field_patterns to apply, use the
    # first that matches.
    if self.field_patterns:
        data_id = None
        fields, message_type = self._parse_field_string(field_string,
                                                        self.compiled_field_patterns)
    # If we were given no explicit field_patterns to use, we need to
    # count on the record having a data_id that lets us figure out
    # which device, and therefore which field_patterns to try.
    else:
        data_id = parsed_record.get(&#39;data_id&#39;, None)
        if data_id is None:
            if not self.quiet:
                logging.warning(&#39;No data id found in record: %s&#39;, record)
            return None
        fields, message_type = self.parse_for_data_id(data_id, field_string)

    # We should now have a dictionary of fields. If not, go home
    if fields is None:
        if not self.quiet:
            logging.warning(&#39;No formats matched field_string &#34;%s&#34;&#39;, field_string)
        return None

    # Some folks want the data_id prepended
    if self.prepend_data_id:
        # This conditional dictates whether fields are stored with just the
        # field_name key, or &lt;data_id&gt;&lt;delimiter&gt;&lt;field_name&gt;
        # Doing some work directly on the fields dict, so we&#39;ll take a copy
        # to loop over.
        fields_copy = fields.copy()
        # Reset the fields dict
        fields = {}
        for field in fields_copy:
            # Determine the new &#34;field_name&#34;
            key = &#39;&#39; + data_id + self.delimiter + field
            # Set the value
            fields[key] = fields_copy[field]

    # Remove raw &#39;field_string&#39; and add parsed &#39;fields&#39; to parsed_record
    del parsed_record[&#39;field_string&#39;]
    parsed_record[&#39;fields&#39;] = fields
    if message_type:
        parsed_record[&#39;message_type&#39;] = message_type

    # If we have parsed fields, see if we also have metadata. Are we
    # supposed to occasionally send it for our variables? Is it time
    # to send it again?
    metadata_fields = {}
    if self.metadata and self.metadata_interval:
        for field_name in fields:
            last_metadata_sent = self.metadata_last_sent.get(field_name, 0)
            time_since_send = timestamp - last_metadata_sent
            if time_since_send &gt; self.metadata_interval:
                field_metadata = self.metadata.get(field_name, None)
                if field_metadata:
                    metadata_fields[field_name] = field_metadata
                    self.metadata_last_sent[field_name] = timestamp
    if metadata_fields:
        metadata = {&#39;fields&#39;: metadata_fields}
    else:
        metadata = None

    if metadata:
        parsed_record[&#39;metadata&#39;] = metadata

    logging.debug(&#39;Created parsed record: %s&#39;, pprint.pformat(parsed_record))

    # What are we going to do with the result we&#39;ve created?
    if self.return_das_record:
        try:
            return DASRecord(data_id=data_id, timestamp=timestamp,
                             message_type=message_type, fields=fields,
                             metadata=metadata)
        except KeyError:
            return None

    elif self.return_json:
        return json.dumps(parsed_record)
    else:
        return parsed_record</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="logger.utils" href="index.html">logger.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="logger.utils.record_parser.RecordParser" href="#logger.utils.record_parser.RecordParser">RecordParser</a></code></h4>
<ul class="">
<li><code><a title="logger.utils.record_parser.RecordParser.parse_for_data_id" href="#logger.utils.record_parser.RecordParser.parse_for_data_id">parse_for_data_id</a></code></li>
<li><code><a title="logger.utils.record_parser.RecordParser.parse_record" href="#logger.utils.record_parser.RecordParser.parse_record">parse_record</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>